# -*- coding: utf-8 -*-
"""Capstone_Projet_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MWoXY6iSYJqds3AF7L0kP4kC2ZIN8Plr
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

#Install Required Libraries
!pip install xgboost shap prophet pmdarima tensorflow --quiet

#Import Libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import warnings
warnings.filterwarnings('ignore')

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
import xgboost as xgb

# For time series
from prophet import Prophet
from statsmodels.tsa.statespace.sarimax import SARIMAX

# For LSTM
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tqdm import tqdm

# Load and Merge Data

train = pd.read_csv("/content/drive/MyDrive/Capstone Project/train.csv", parse_dates=["Date"])
store = pd.read_csv("/content/drive/MyDrive/Capstone Project/store.csv")

# Merge datasets
df = pd.merge(train, store, on="Store")
df.head()

# Clean & Preprocess Data
# Fill missing values

df['CompetitionDistance'].fillna(df['CompetitionDistance'].median(), inplace=True)
df['StateHoliday'] = df['StateHoliday'].astype(str)

#Feature Engineering

# Create date-related features

df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day
df['DayOfWeek'] = df['Date'].dt.dayofweek
df['WeekOfYear'] = df['Date'].dt.isocalendar().week
df['IsWeekend'] = df['DayOfWeek'].isin([5, 6])
df['PromoRunning'] = (df['Promo'] == 1) & (df['Open'] == 1)

df.head()

# EDA – Visualize Sales Trends

plt.figure(figsize=(10,4))
sns.histplot(df['Sales'], bins=50, kde=True)
plt.title('Sales Distribution')
plt.show()

# Sales over time
df.groupby('Date')['Sales'].sum().plot(figsize=(15, 5), title='Total Sales Over Time')
plt.ylabel('Sales')
plt.show()

# Baseline Model – Linear Regression

model_df = df[(df['Open'] == 1) & (df['Sales'] > 0)].copy()

features = ['Store', 'Promo', 'DayOfWeek', 'Month', 'Year']
X = model_df[features]
y = model_df['Sales']

X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)

lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)

print("Linear Regression RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))
print("Linear Regression MAE:", mean_absolute_error(y_test, y_pred))

# Random Forest Model with Feature Importance

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print("Random Forest RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_rf)))
print("Random Forest MAE:", mean_absolute_error(y_test, y_pred_rf))

importances = pd.Series(rf.feature_importances_, index=features)
importances.sort_values().plot(kind='barh')
plt.title("Random Forest Feature Importance")
plt.show()

# XGBoost Model

xgb_model = xgb.XGBRegressor(objective="reg:squarederror", n_estimators=100)
xgb_model.fit(X_train, y_train)

y_pred_xgb = xgb_model.predict(X_test)

print("XGBoost RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_xgb)))
print("XGBoost MAE:", mean_absolute_error(y_test, y_pred_xgb))

# Feature Importance
xgb.plot_importance(xgb_model, importance_type='gain')
plt.title("XGBoost Feature Importance (Gain)")
plt.show()

explainer = shap.Explainer(xgb_model)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test)

# Store-Level Performance Analysis (XGBoost)

X_test_with_store = X_test.copy()
X_test_with_store['Store'] = model_df.loc[X_test.index, 'Store']
X_test_with_store['Actual'] = y_test
X_test_with_store['Predicted_XGB'] = y_pred_xgb

store_mae = X_test_with_store.groupby('Store').apply(lambda x: mean_absolute_error(x['Actual'], x['Predicted_XGB'])).reset_index()
store_mae.columns = ['Store', 'MAE_XGB']

plt.figure(figsize=(12,6))
plt.bar(store_mae['Store'], store_mae['MAE_XGB'])
plt.xlabel("Store")
plt.ylabel("MAE")
plt.title("XGBoost MAE by Store")
plt.show()

# Prophet: Time Series Forecasting for Store 1

store1_df = df[(df['Store'] == 1) & (df['Open'] == 1) & (df['Sales'] > 0)]
store1_ts = store1_df.groupby('Date')['Sales'].sum().reset_index()
prophet_df = store1_ts.rename(columns={'Date':'ds','Sales':'y'})

model_p = Prophet()
model_p.fit(prophet_df)

future = model_p.make_future_dataframe(periods=60)
forecast = model_p.predict(future)

model_p.plot(forecast)
plt.title("Prophet Forecast - Store 1")
plt.show()

model_p.plot_components(forecast)
plt.show()

# SARIMA: Time Series Forecasting for Store 1

store_ts_arima = store1_ts.set_index('Date')['Sales']

model_sarima = SARIMAX(store_ts_arima,
                       order=(1,1,1),
                       seasonal_order=(1,1,1,7),
                       enforce_stationarity=False,
                       enforce_invertibility=False)

results = model_sarima.fit()
print(results.summary())

forecast = results.get_forecast(steps=60)
forecast_ci = forecast.conf_int()
forecast_index = pd.date_range(start=store_ts_arima.index[-1] + pd.Timedelta(days=1), periods=60)
forecast_values = forecast.predicted_mean

plt.figure(figsize=(12,5))
plt.plot(store_ts_arima, label="Observed")
plt.plot(forecast_index, forecast_values, label="SARIMA Forecast")
plt.fill_between(forecast_index, forecast_ci.iloc[:,0], forecast_ci.iloc[:,1], color='gray', alpha=0.3)
plt.title("SARIMA Forecast - Store 1")
plt.legend()
plt.show()

# LSTM Model Development


sequence_length = 30
X_all, y_all = [], []

store_ids = df['Store'].unique()

for store in tqdm(store_ids):
    store_data = df[(df['Store'] == store) & (df['Open'] == 1) & (df['Sales'] > 0)].copy()
    store_data = store_data.sort_values('Date')

    scaler = MinMaxScaler()
    scaled_sales = scaler.fit_transform(store_data[['Sales']])

    for i in range(len(scaled_sales) - sequence_length):
        X_all.append(scaled_sales[i:i+sequence_length])
        y_all.append(scaled_sales[i+sequence_length])

X_all = np.array(X_all)
y_all = np.array(y_all)

split = int(len(X_all) * 0.8)
X_train, X_test = X_all[:split], X_all[split:]
y_train, y_test = y_all[:split], y_all[split:]

model = Sequential([
    LSTM(50, activation='relu', input_shape=(sequence_length,1)),
    Dense(1)
])
model.compile(optimizer='adam', loss='mse')
model.fit(X_train, y_train, epochs=5, batch_size=128)

y_pred = model.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)

print(f"LSTM (Full Dataset) RMSE (scaled): {rmse:.4f}")
print(f"LSTM (Full Dataset) MAE (scaled): {mae:.4f}")

# Plot sample predictions
plt.figure(figsize=(12, 5))
plt.plot(y_test[:100], label='Actual')
plt.plot(y_pred[:100], label='Predicted')
plt.title("LSTM - Scaled Forecasts (Full Dataset)")
plt.legend()
plt.show()

# LSTM Model Development for Store 1

# Define sequence length
SEQ_LENGTH = 30

# 2. Prepare dataset for Store 1
# Filter only open stores with valid sales for Store 1
store_data = df[(df['Store'] == 1) & (df['Open'] == 1) & (df['Sales'] > 0)].copy()
store_data = store_data.sort_values('Date')

# 3. Normalize and generate sequences for Store 1
scaler = MinMaxScaler()
scaled_sales = scaler.fit_transform(store_data[['Sales']])

X, y = [], []

# Create sequences
for i in range(len(scaled_sales) - SEQ_LENGTH):
    X.append(scaled_sales[i:i + SEQ_LENGTH])
    y.append(scaled_sales[i + SEQ_LENGTH])

X = np.array(X)
y = np.array(y)

# 4. Train-test split (for Store 1 data)
split = int(len(X) * 0.8)
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]


# 5. Build LSTM model
model = Sequential()
model.add(LSTM(50, activation='relu', input_shape=(SEQ_LENGTH, 1)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')

# 6. Train model
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1)

# 7. Plot training & validation loss
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='validation')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()

# 8. Predict on test set
y_pred = model.predict(X_test)

# Inverse transform to original scale
y_test_inv = scaler.inverse_transform(y_test)
y_pred_inv = scaler.inverse_transform(y_pred)

# 9. Evaluate model
from sklearn.metrics import mean_squared_error, mean_absolute_error

rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))
mae = mean_absolute_error(y_test_inv, y_pred_inv)

print("LSTM RMSE:", rmse)
print("LSTM MAE:", mae)

# 10. Plot actual vs predicted
plt.figure(figsize=(12,5))
plt.plot(y_test_inv, label='Actual')
plt.plot(y_pred_inv, label='Predicted')
plt.title('LSTM Forecast vs Actual - Store 1')
plt.xlabel('Time Step')
plt.ylabel('Sales')
plt.legend()
plt.show()

# Store model results with final LSTM metrics

results = {
    'Model': ['Linear Regression', 'XGBoost', 'LSTM', 'Random Forest'],
    'RMSE': [2818.89, 2524.01, 709.76, 1242.76],
    'MAE': [2005.88, 1862.88, 592.24, 854.99]
}


# Create dataframe
results_df = pd.DataFrame(results)
print("Model Performance Summary")
print(results_df)

# Visualize RMSE comparison
plt.figure(figsize=(8,5))
sns.barplot(data=results_df, x='Model', y='RMSE')
plt.title('Model RMSE Comparison')
plt.ylabel('RMSE')
plt.xlabel('Model')
plt.xticks(rotation=45)
plt.show()

# Visualize MAE comparison
plt.figure(figsize=(8,5))
sns.barplot(data=results_df, x='Model', y='MAE')
plt.title('Model MAE Comparison')
plt.ylabel('MAE')
plt.xlabel('Model')
plt.xticks(rotation=45)
plt.show()

# Interpretation prompt

# - LSTM has significantly lower RMSE and MAE compared to Linear Regression and XGBoost.
# - This suggests LSTM captures temporal patterns in sales data better.
# - However, LSTM requires higher computational resources and longer training time.
# - XGBoost remains strong for tabular feature-based regression tasks.